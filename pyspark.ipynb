{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,TimestampType\n",
    "from pyspark.sql.functions import from_json, col, window, avg, count\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaCassandraIntegration\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,\"\n",
    "            \"com.datastax.spark:spark-cassandra-connector_2.12:3.3.0\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"KafkaTemperatureTopic\") \\\n",
    "#         .config(\"spark.jars.packages\", \n",
    "#             \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,\"\n",
    "#             \"com.datastax.spark:spark-cassandra-connector_2.12:3.3.0\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Read data from Kafka\n",
    "temperature_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"temperature\") \\\n",
    "    .load()\n",
    "\n",
    "humidity_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"humidity\") \\\n",
    "    .load()\n",
    "\n",
    "# Define the schema for the JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"measurementId\", StringType(), True,),\n",
    "    StructField(\"reading\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"device\", IntegerType(), True )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast key and value to string\n",
    "temperature_messages = temperature_stream.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "humidity_messages = humidity_stream.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "\n",
    "# Parseanding\n",
    "temperature_stream = temperature_messages.withColumn(\"parsed_value\", from_json(col(\"value\"), schema))\n",
    "humidity_stream = humidity_messages.withColumn(\"parsed_value\", from_json(col(\"value\"), schema))\n",
    "\n",
    "#temperature_stream = temperature_stream.withColumn(\"parsed_value.timestamp\", col(\"parsed_value.timestamp\").cast(\"timestamp\"))\n",
    "temperature_stream = temperature_stream.withColumn(\"timestamp\", col(\"parsed_value.timestamp\"))\n",
    "humidity_stream = humidity_stream.withColumn(\"timestamp\", col(\"parsed_value.timestamp\"))\n",
    "\n",
    "#Le damos un watermark de 5 segundos\n",
    "\n",
    "temperature_stream_with_watermark = temperature_stream.withWatermark(\"timestamp\", \"1 seconds\")\n",
    "humidity_stream_with_watermark = humidity_stream.withWatermark(\"timestamp\", \"1 seconds\")\n",
    "\n",
    "\n",
    "# Extraemos solo las columnas que nos son utiles para los plots\n",
    "\n",
    "temperature_processed_stream = temperature_stream_with_watermark.select(\n",
    "    col(\"parsed_value.measurementId\").alias(\"id\"),\n",
    "    col(\"timestamp\").alias(\"timestamp_t\"),\n",
    "    col(\"parsed_value.reading\").alias(\"temperature\"),\n",
    "    col(\"parsed_value.device\").alias(\"device\")\n",
    ")\n",
    "\n",
    "humidity_processed_stream = humidity_stream_with_watermark.select(\n",
    "    col(\"parsed_value.measurementId\").alias(\"id\"),\n",
    "    col(\"timestamp\").alias(\"timestamp_h\"),\n",
    "    col(\"parsed_value.reading\").alias(\"humidity\"),\n",
    "    col(\"parsed_value.device\").alias(\"device\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cassandra Keyspace\n",
    "\n",
    "CREATE KEYSPACE IF NOT EXISTS postgradoBigData \n",
    "WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};\n",
    "\n",
    "#Cassandra Table\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS postgradoBigData.sensor_data (\n",
    "    window_start TIMESTAMP PRIMARY KEY,\n",
    "    temperature_average DOUBLE,\n",
    "    humidity_average DOUBLE\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joineamos ambos DF por el campo que tienen en comun: ID\n",
    "\n",
    "joined_stream = temperature_processed_stream.join(humidity_processed_stream, on=\"id\", how=\"inner\")\n",
    "\n",
    "#filtramos para solo mostrar las columnas que queremos y usamos el nombre de la columna renombrada: \n",
    "\n",
    "joined_stream_cols = joined_stream.select(\n",
    "    col(\"timestamp_T\"), \\\n",
    "    col(\"temperature\"), \\\n",
    "    col(\"humidity\")\n",
    ")\n",
    "\n",
    "joined_stream_windowed = joined_stream_cols.groupBy(window(col(\"timestamp_T\"), \"1 seconds\"))    \\\n",
    "    .agg(avg(\"temperature\").alias(\"temperature_average\"),\n",
    "        (avg(\"humidity\").alias(\"humidity_average\")))\n",
    "\n",
    "\n",
    "df_to_write = joined_stream_windowed.withColumn(\"window_start\", col(\"window.start\")) \\\n",
    "    .select(\"window_start\", \"temperature_average\", \"humidity_average\")\n",
    "\n",
    "#mandemos todo a Cassandra \n",
    "\n",
    "def write_to_cassandra(batch_df, batch_id):\n",
    "    batch_df.withColumn(\"window_start\", col(\"window.start\")) \\\n",
    "        .select(\"window_start\", \"temperature_average\", \"humidity_average\") \\\n",
    "        .write \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .options(table=\"sensor_data\", keyspace=\"postgradobigdata\") \\\n",
    "        .save()\n",
    "\n",
    "joined_stream_windowed.writeStream \\\n",
    "    .foreachBatch(write_to_cassandra) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n",
    "\n",
    "\n",
    "# query = joined_stream_windowed.writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#      .start()\n",
    "\n",
    "#query.awaitTermination()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
